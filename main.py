# main.py (ä¿®æ”¹å)


import torch, time, os, shutil
import models, utils
import numpy as np
import pandas as pd
from torch import nn, optim
from torch.utils.data import DataLoader
from dataset import FECGDataset
from config import config
import matplotlib.pyplot as plt
from torch.optim import lr_scheduler
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm
import argparse
from clinical_evaluator import robust_detect_r_peaks, calculate_qrs_performance, calculate_fhr_error
SAMPLING_RATE = 200

# ã€æ–°å¢ã€‘å¯¼å…¥è®¡ç®— RÂ² å’Œ MSE çš„åº“
from sklearn.metrics import r2_score, mean_squared_error

import warnings
warnings.filterwarnings("ignore")

os.environ["CUDA_DEVICE_OEDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="1"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

torch.manual_seed(41)
torch.cuda.manual_seed(41)

# ä¿å­˜æ¨¡å‹çš„å‡½æ•°ä¿æŒä¸å˜
def save_ckpt(state, is_best, model_save_dir):
    current_w = os.path.join(model_save_dir, config.current_w)
    best_w = os.path.join(model_save_dir, config.best_w)
    torch.save(state, current_w)
    if is_best: shutil.copyfile(current_w, best_w)

# train_epoch å‡½æ•°ä¿æŒä¸å˜
def train_epoch(model, optimizer, criterion, scheduler, train_dataloader, show_interval=100):
    model.train()
    losses = []
    total = 0
    tbar = tqdm(train_dataloader)
    for i, (inputs, target) in enumerate(tbar):      
        data = inputs.to(device)
        labelt = target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output,labelt.to(torch.float32))
        loss.backward()
        optimizer.step()
        scheduler.step()
        losses.append(loss.item())
    tbar.close()       
    for i in range(len(losses)):
        total = total + losses[i]
    total /= len(losses)
    # ã€ä¿®æ”¹ã€‘ç§»é™¤æ— æ„ä¹‰çš„f1åˆ†æ•°
    return total

# val_epoch å‡½æ•°ä¿æŒä¸å˜
def val_epoch(model, optimizer, criterion, scheduler, val_dataloader, show_interval=100):
    model.eval()
    losses = []
    total = 0
    tbar = tqdm(val_dataloader)
    for i, (inputs, target) in enumerate(tbar):     
        data = inputs.to(device)
        labelt = target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output,labelt.to(torch.float32))
        losses.append(loss.item())
    for i in range(len(losses)):
        total = total + losses[i]
    total /= len(losses)
    # ã€ä¿®æ”¹ã€‘ç§»é™¤æ— æ„ä¹‰çš„f1åˆ†æ•°
    return total

# ã€æ–°å¢ã€‘ç‹¬ç«‹çš„æµ‹è¯•å‡½æ•°
def test_epoch(model, test_dataloader, model_name):
    """
    åœ¨ç‹¬ç«‹çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½ï¼ˆåŒ…å«ä¿¡å·çº§å’Œä¸´åºŠçº§æŒ‡æ ‡ï¼‰
    """
    model.eval()
    all_targets = []
    all_outputs = []
    
    print("\n" + "="*20)
    print("å¼€å§‹åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    print(f"æµ‹è¯•æ•°æ®æ–‡ä»¶: r10.edf")
    print("="*20)

    with torch.no_grad():
        tbar = tqdm(test_dataloader)
        for i, (inputs, target) in enumerate(tbar):
            data = inputs.to(device, dtype=torch.float32)
            labelt = target.to(device, dtype=torch.float32)
            output = model(data)
            all_targets.append(labelt.cpu().numpy())
            all_outputs.append(output.cpu().numpy())

    # --- è®¡ç®—å¹¶æ‰“å°æœ€ç»ˆçš„è¯„ä¼°æŒ‡æ ‡ ---
    # æ‹¼æ¥æˆä¸€ä¸ªå¤§æ•°ç»„
    all_targets_np = np.vstack(all_targets)
    all_outputs_np = np.vstack(all_outputs)
    targets_flat = all_targets_np.flatten()
    outputs_flat = all_outputs_np.flatten()

    # ä¿¡å·çº§æŒ‡æ ‡
    mse = mean_squared_error(targets_flat, outputs_flat)
    rmse = np.sqrt(mse)
    r2 = r2_score(targets_flat, outputs_flat)

    # ä¸´åºŠçº§æŒ‡æ ‡
    true_peaks = robust_detect_r_peaks(targets_flat, sampling_rate=SAMPLING_RATE)
    pred_peaks = robust_detect_r_peaks(outputs_flat, sampling_rate=SAMPLING_RATE)
    qrs_performance = calculate_qrs_performance(true_peaks, pred_peaks, tolerance_ms=20, sampling_rate=SAMPLING_RATE)
    fhr_mae = calculate_fhr_error(true_peaks, pred_peaks, sampling_rate=SAMPLING_RATE, tolerance_ms=100)

    print("\n--- âœ… æœ€ç»ˆæµ‹è¯•ç»“æœ ---")
    print("--- ä¿¡å·çº§æŒ‡æ ‡ ---")
    print(f"å‡æ–¹è¯¯å·® (MSE): {mse:.6f}")
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.6f}")
    print(f"å†³å®šç³»æ•° (RÂ²): {r2:.6f}")
    print("\n--- ä¸´åºŠçº§æŒ‡æ ‡ ---")
    print(f"QRS F1-Score: {qrs_performance['f1_score']:.4f} (Se={qrs_performance['sensitivity']:.4f}, P+={qrs_performance['precision']:.4f})")
    print(f"èƒå¿ƒç‡è¯¯å·® (FHR MAE): {fhr_mae:.4f} BPM")
    print("------------------------\n")


def weights_init_normal(m):
    # ... (æ­¤å‡½æ•°æ— å˜åŒ–) ...
    if isinstance(m, nn.Conv1d):
        tanh_gain = nn.init.calculate_gain('tanh')
        nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain)

def train(args):
    model = getattr(models, config.model_name)(output_size=300)
    args.ckpt = None
    if args.ckpt and not args.resume:
        state = torch.load(args.ckpt, map_location='cpu')
        model.load_state_dict(state['state_dict'])
        print('train with pretrained weight')
   
    model.apply(weights_init_normal)
    model = model.to(device)
    
    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ•°æ®åŠ è½½ ---
    # å®šä¹‰æ¸…æ™°çš„æ•°æ®æ–‡ä»¶ç´¢å¼•
    TRAIN_FILES = [1, 2, 3] # r04, r07, r08
    VAL_FILE = [0]          # r01
    TEST_FILE = [4]         # r10
    
    print("--- æ­£åœ¨å‡†å¤‡æ•°æ®é›† ---")
    # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    train_dataset = FECGDataset(data_path=config.train_dir, file_indices=TRAIN_FILES)
    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)
    print(f"è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬, æ¥è‡ªæ–‡ä»¶ç´¢å¼• {TRAIN_FILES}")

    # åˆ›å»ºéªŒè¯æ•°æ®é›†
    val_dataset = FECGDataset(data_path=config.val_dir, file_indices=VAL_FILE)
    val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, num_workers=0)
    print(f"éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬, æ¥è‡ªæ–‡ä»¶ç´¢å¼• {VAL_FILE}")

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = FECGDataset(data_path=config.test_dir, file_indices=TEST_FILE)
    test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=0)
    print(f"æµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬, æ¥è‡ªæ–‡ä»¶ç´¢å¼• {TEST_FILE}")
    print("----------------------\n")
 
    optimizer = optim.Adam(model.parameters(), lr=config.lr)  
    criterion = nn.MSELoss()
    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=800, gamma=0.1)
    
    model_save_dir = '%s/%s' % (config.ckpt, config.model_name)
    if args.ex: model_save_dir += args.ex
    
    # ã€ä¿®æ”¹ã€‘ç§»é™¤äº† best_f1, ä½¿ç”¨ min_loss ä½œä¸ºæ ‡å‡†
    min_loss = float('inf')
    lr = config.lr
    start_epoch = 1
    stage = 1
    
    print("--- ğŸš€ å¼€å§‹è®­ç»ƒ ---")
    for epoch in range(start_epoch, config.max_epoch + 1):
        since = time.time()
        
        # ã€ä¿®æ”¹ã€‘å‡½æ•°è¿”å›å€¼å·²æ›´æ–°
        train_loss = train_epoch(model, optimizer, criterion, exp_lr_scheduler, train_dataloader, show_interval=epoch)
        val_loss = val_epoch(model, optimizer, criterion, exp_lr_scheduler, val_dataloader, show_interval=epoch)
        
        print('\n')
        # ã€ä¿®æ”¹ã€‘æ›´æ–°æ‰“å°ä¿¡æ¯ï¼Œç§»é™¤f1
        print('#epoch:%02d stage:%d train_loss:%.4e val_loss:%0.4e time:%s\n'
              % (epoch, stage, train_loss, val_loss, utils.print_time_cost(since)))
              
        is_best = val_loss < min_loss
        min_loss = min(val_loss, min_loss)
        
        # ã€ä¿®æ”¹ã€‘stateå­—å…¸ä¸­ç§»é™¤f1
        state = {"state_dict": model.state_dict(), "epoch": epoch, "loss": val_loss, 'lr': lr, 'stage': stage}
        save_ckpt(state, is_best, model_save_dir)

    print("--- ğŸ‰ è®­ç»ƒå®Œæˆ ---")

    # --- ã€æ–°å¢ã€‘è®­ç»ƒåè‡ªåŠ¨å¼€å§‹æµ‹è¯• ---
    # åŠ è½½è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æœ€ä½³æ¨¡å‹
    # --- ã€æ–°å¢ã€‘è®­ç»ƒåè‡ªåŠ¨å¼€å§‹æµ‹è¯• ---
    best_w_path = os.path.join(model_save_dir, config.best_w)
    if os.path.exists(best_w_path):
        print(f"\nåŠ è½½æ€§èƒ½æœ€ä½³çš„æ¨¡å‹ '{config.best_w}' ç”¨äºæœ€ç»ˆæµ‹è¯•...")
        best_model_state = torch.load(best_w_path, map_location=device)
        model.load_state_dict(best_model_state['state_dict'])
        # ã€ä¿®æ”¹ã€‘è°ƒç”¨æ›´æ–°åçš„æµ‹è¯•å‡½æ•°
        test_epoch(model, test_dataloader, config.model_name)
    else:
        print("æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡æµ‹è¯•ã€‚")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('command', help='ä»…æ”¯æŒ "train" å‘½ä»¤')
    parser.add_argument("--ex", type=str, help="experience name")
    # ä»¥ä¸‹å‚æ•°åœ¨å½“å‰é€»è¾‘ä¸­æœªä½¿ç”¨ï¼Œä½†ä¿ç•™
    parser.add_argument("--ckpt", type=str, help="the path of model weight file")
    parser.add_argument("--resume", action='store_true', default=False)
    args = parser.parse_args()

    if args.command == 'train':
        train(args)
    else:
        print("æ— æ•ˆå‘½ä»¤ï¼Œè¯·ä½¿ç”¨ 'train'")